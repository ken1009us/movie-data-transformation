{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2ce40ed-efcd-4674-9a7f-a3f4e8c36951",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Movie Data Transformation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18629caa-df19-4ffc-b6e8-59979fd34bc7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "592debe9-55a9-491a-9fa9-c5c96468c016",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a793396b-4d8d-479c-b57a-4b2f671d174a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b171dc5-d0df-44d0-a3e1-43db911112c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MovieDataTransformation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "dbfs_path = \"/dbfs/FileStore/tables/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "187cb2fd-e513-4a2e-b903-c358afdac310",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define Schema and Read JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a1d26de-6797-464b-a787-80cd7e2938f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType\n",
    "\n",
    "def read_json(file_path):\n",
    "    schema = StructType([\n",
    "        StructField(\"Id\", IntegerType(), True),\n",
    "        StructField(\"Title\", StringType(), True),\n",
    "        StructField(\"Overview\", StringType(), True),\n",
    "        StructField(\"Tagline\", StringType(), True),\n",
    "        StructField(\"Budget\", DoubleType(), True),\n",
    "        StructField(\"Revenue\", DoubleType(), True),\n",
    "        StructField(\"ImdbUrl\", StringType(), True),\n",
    "        StructField(\"TmdbUrl\", StringType(), True),\n",
    "        StructField(\"PosterUrl\", StringType(), True),\n",
    "        StructField(\"BackdropUrl\", StringType(), True),\n",
    "        StructField(\"OriginalLanguage\", StringType(), True),\n",
    "        StructField(\"ReleaseDate\", StringType(), True),\n",
    "        StructField(\"RunTime\", IntegerType(), True),\n",
    "        StructField(\"Price\", DoubleType(), True),\n",
    "        StructField(\"CreatedDate\", StringType(), True),\n",
    "        StructField(\"UpdatedDate\", StringType(), True),\n",
    "        StructField(\"UpdatedBy\", StringType(), True),\n",
    "        StructField(\"CreatedBy\", StringType(), True),\n",
    "        StructField(\"genres\", ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"id\", IntegerType(), True),\n",
    "                StructField(\"name\", StringType(), True)\n",
    "            ])\n",
    "        ), True)\n",
    "    ])\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return spark.createDataFrame(data[\"movie\"], schema=schema)\n",
    "\n",
    "\n",
    "movie_files = [\"movie_0.json\", \"movie_1.json\", \"movie_2.json\", \"movie_3.json\",\n",
    "               \"movie_4.json\", \"movie_5.json\", \"movie_6.json\", \"movie_7.json\"]\n",
    "\n",
    "dataframes = [read_json(dbfs_path + file) for file in movie_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d71a9c-8e9d-4341-8218-1842bfc2859e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c33025b9-130e-435b-b139-889c827fb84d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    quarantined_df = df.filter(df[\"RunTime\"] < 0)\n",
    "    clean_df = df.filter(df[\"RunTime\"] >= 0)\n",
    "    clean_df = clean_df.withColumn(\"Quarantined\", df[\"RunTime\"] < 0)\n",
    "    clean_df = clean_df.withColumn(\"RunTime\", F.abs(df[\"RunTime\"]))\n",
    "    clean_df = clean_df.withColumn(\"Budget\", F.when(df[\"Budget\"] < 1000000, 1000000).otherwise(df[\"Budget\"]))\n",
    "    \n",
    "    return clean_df, quarantined_df\n",
    "\n",
    "def save_as_delta(df, table_name):\n",
    "    delta_path = dbfs_path + table_name\n",
    "\n",
    "    try:\n",
    "        dbutils.fs.rm(delta_path, recurse=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while removing existing folder: {e}\")\n",
    "    \n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "def bronze_to_silver(dataframes):\n",
    "    silver_tables = []\n",
    "    quarantined_records = []\n",
    "\n",
    "    for i, df in enumerate(dataframes):\n",
    "        clean_df, quarantined_df = clean_data(df)\n",
    "        silver_tables.append(clean_df)\n",
    "        quarantined_records.append(quarantined_df)\n",
    "\n",
    "        save_as_delta(clean_df, f\"silver_table_{i}\")\n",
    "\n",
    "    combined_quarantined_df = quarantined_records[0]\n",
    "    for q_df in quarantined_records[1:]:\n",
    "        combined_quarantined_df = combined_quarantined_df.union(q_df)\n",
    "    save_as_delta(combined_quarantined_df, \"quarantined_records\")\n",
    "\n",
    "    return silver_tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53cada45-854f-4c39-8acd-e22cb4274b39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Lookup Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45e18b3d-251c-47c9-8a9b-76377e2d6db3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(dataframes):\n",
    "    genres = set()\n",
    "    languages = set()\n",
    "\n",
    "    for df in dataframes:\n",
    "        if df.count() > 0:\n",
    "            genres.update([genre[\"id\"] for row in df.select(F.explode(\"genres\")).distinct().collect() for genre in row[\"col\"] if isinstance(row[\"col\"], list)])\n",
    "            languages.update(df.select(\"OriginalLanguage\").distinct().rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "    if not genres:\n",
    "        genres.add(0)\n",
    "    if not languages:\n",
    "        languages.add(\"Unknown\")\n",
    "\n",
    "    genres_df = spark.createDataFrame([(genre,) for genre in sorted(genres)], [\"GenreId\"])\n",
    "    languages_df = spark.createDataFrame([(lang,) for lang in sorted(languages)], [\"Language\"])\n",
    "\n",
    "    save_as_delta(genres_df, \"genres_lookup\")\n",
    "    save_as_delta(languages_df, \"languages_lookup\")\n",
    "\n",
    "    return genres_df, languages_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ab1070e-cd1f-4812-bce2-5b71e2684d14",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Fix Missing Genre Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cdbd85f-d287-4c98-af4a-e0a957e89a70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, struct\n",
    "\n",
    "def fix_missing_genre_names(df, genres_lookup):\n",
    "    df = df.alias(\"movies\")\n",
    "    genres_lookup = genres_lookup.alias(\"lookup\")\n",
    "\n",
    "    exploded_df = df.withColumn(\"exploded_genres\", explode(\"genres\"))\n",
    "    joined_df = exploded_df.join(\n",
    "        genres_lookup,\n",
    "        exploded_df[\"exploded_genres.id\"] == genres_lookup[\"GenreId\"],\n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "    fixed_genres = joined_df.withColumn(\n",
    "        \"genres\",\n",
    "        struct(\n",
    "            \"exploded_genres.id\",\n",
    "            F.coalesce(\"exploded_genres.name\", F.col(\"lookup.GenreId\").cast(\"string\"))\n",
    "        )\n",
    "    ).drop(\"exploded_genres\")\n",
    "\n",
    "    return fixed_genres.groupBy(*df.columns).agg(F.collect_list(\"genres\").alias(\"genres\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfd7c0ad-9197-460e-8a64-4a5dc851cf26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Handle Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4ffc796-228c-4dad-989d-ebfd8506ec0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def handle_duplicates(silver_tables):\n",
    "    # Assuming the 'status' column is present to indicate new or loaded records\n",
    "    unique_records = []\n",
    "    for df in silver_tables:\n",
    "        df = df.withColumn(\"status\", F.lit(\"loaded\"))\n",
    "        unique_records.append(df)\n",
    "\n",
    "    combined_df = reduce(lambda x, y: x.union(y), unique_records)\n",
    "    deduplicated_df = combined_df.dropDuplicates([\"Id\"])\n",
    "    return deduplicated_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c51020a1-37b7-4582-80ea-e7cc0e0e38ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4317350f-1001-44f4-b11a-c1d8731dcd13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "genres_lookup, languages_lookup = create_lookup_tables(dataframes)\n",
    "fixed_silver_tables = [fix_missing_genre_names(df, genres_lookup) for df in silver_tables if df.count() > 0]\n",
    "deduplicated_df = handle_duplicates(fixed_silver_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a405282f-ad46-4474-b3a0-c6942c8dac84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Save Final Silver Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "913c3f29-bcfb-427e-87e4-50effd993aa9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'genres' column does not exist in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "# Check if the \"genres\" column exists in the DataFrame\n",
    "if \"genres\" in deduplicated_df.columns:\n",
    "    # Flatten the \"genres\" column\n",
    "    deduplicated_df = deduplicated_df.withColumn(\"genres\", explode(col(\"genres\")))\n",
    "    deduplicated_df = deduplicated_df.select(\"*\", col(\"genres.id\").alias(\"genre_id\"), col(\"genres.name\").alias(\"genre_name\")).drop(\"genres\")\n",
    "else:\n",
    "    print(\"The 'genres' column does not exist in the DataFrame.\")\n",
    "\n",
    "# Save the flattened DataFrame as a Delta table\n",
    "save_as_delta(deduplicated_df, \"silver_movie_table\")\n",
    "save_as_delta(genres_lookup, \"silver_genres_lookup\")\n",
    "save_as_delta(languages_lookup, \"silver_languages_lookup\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "movie-assignment",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
